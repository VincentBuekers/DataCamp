{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/Users/Vincent/Desktop/Python/DataCamp/Data')\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "party                  0\n",
      "infants               12\n",
      "water                 48\n",
      "budget                11\n",
      "physician             11\n",
      "salvador              15\n",
      "religious             11\n",
      "satellite             14\n",
      "aid                   15\n",
      "missile               22\n",
      "immigration            7\n",
      "synfuels              20\n",
      "education             31\n",
      "superfund             25\n",
      "crime                 17\n",
      "duty_free_exports     28\n",
      "eaa_rsa              104\n",
      "dtype: int64\n",
      "Shape of Original DataFrame: (434, 17)\n",
      "Shape of DataFrame After Dropping All Rows with Missing Values: (232, 17)\n"
     ]
    }
   ],
   "source": [
    "votes = pd.read_csv('house-votes-84.csv')\n",
    "votes.columns = ['party','infants', 'water', 'budget', 'physician', 'salvador', 'religious',\n",
    "       'satellite', 'aid', 'missile', 'immigration', 'synfuels', 'education',\n",
    "       'superfund', 'crime', 'duty_free_exports', 'eaa_rsa']\n",
    "votes = votes.replace(to_replace=['n', 'y'], value=[0, 1])\n",
    "#Convert '?' to NaN\n",
    "votes[votes == \"?\"] = np.nan\n",
    "\n",
    "# Print the number of NaNs\n",
    "print(votes.isnull().sum())\n",
    "# Print shape of original DataFrame\n",
    "print(\"Shape of Original DataFrame: {}\".format(votes.shape))\n",
    "\n",
    "# Drop missing values and print shape of new DataFrame\n",
    "votes = votes.dropna()\n",
    "print(\"Shape of DataFrame After Dropping All Rows with Missing Values: {}\".format(votes.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length_of_time    143\n",
      "state             419\n",
      "type              159\n",
      "dtype: int64\n",
      "(4283, 11)\n"
     ]
    }
   ],
   "source": [
    "ufo = pd.read_csv('ufo_sightings_large.csv')\n",
    "\n",
    "# Check how many values are missing in the length_of_time, state, and type columns\n",
    "print(ufo[[\"length_of_time\", \"state\", \"type\"]].isnull().sum())\n",
    "\n",
    "# Keep only rows where length_of_time, state, and type are not null\n",
    "ufo_no_missing = ufo[ufo[\"length_of_time\"].notnull() & \n",
    "          ufo[\"state\"].notnull() & \n",
    "          ufo[\"type\"].notnull()]\n",
    "\n",
    "print(ufo_no_missing.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Date-time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    length_of_time  minutes\n",
      "0          2 weeks      2.0\n",
      "1           30sec.     30.0\n",
      "3  about 5 minutes      NaN\n",
      "4                2      2.0\n",
      "5       10 minutes     10.0\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def return_minutes(time_string):\n",
    "\n",
    "    # Use \\d+ to grab digits\n",
    "    pattern = re.compile(r\"\\d+\")\n",
    "    \n",
    "    # Use match on the pattern and column\n",
    "    num = re.match(pattern, time_string)\n",
    "    if num is not None:\n",
    "        return int(num.group(0))\n",
    "        \n",
    "# Apply the extraction to the length_of_time column\n",
    "ufo_no_missing[\"minutes\"] = ufo_no_missing[\"length_of_time\"].apply(lambda row: return_minutes(row))\n",
    "\n",
    "# Take a look at the head of both of the columns\n",
    "print(ufo_no_missing[[\"length_of_time\",\"minutes\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seconds    float64\n",
      "date        object\n",
      "dtype: object\n",
      "                 date  month  year\n",
      "0 2011-11-03 19:21:00     11  2011\n",
      "1 2004-10-03 19:05:00     10  2004\n",
      "2 2009-09-25 21:00:00      9  2009\n",
      "3 2002-11-21 05:45:00     11  2002\n",
      "4 2010-08-19 12:55:00      8  2010\n"
     ]
    }
   ],
   "source": [
    "# Check the column types\n",
    "print(ufo[[\"seconds\", \"date\"]].dtypes)\n",
    "\n",
    "# Change the type of seconds to float\n",
    "ufo[\"seconds\"] = ufo[\"seconds\"].astype(float)\n",
    "# Change the date column to type datetime\n",
    "ufo[\"date\"] = pd.to_datetime(ufo[\"date\"])\n",
    "\n",
    "# Extract the month from the date column\n",
    "ufo[\"month\"] = ufo[\"date\"].apply(lambda row: row.month)\n",
    "\n",
    "# Extract the year from the date column\n",
    "ufo[\"year\"] = ufo[\"date\"].apply(lambda row: row.year)\n",
    "\n",
    "print(ufo[[\"date\", \"month\", \"year\"]].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stratified sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Strengthening Communities    307\n",
       "Helping Neighbors in Need    119\n",
       "Education                     92\n",
       "Health                        52\n",
       "Environment                   32\n",
       "Emergency Preparedness        15\n",
       "Name: category_desc, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "volunteer['category_desc'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strengthening Communities    230\n",
      "Helping Neighbors in Need     89\n",
      "Education                     69\n",
      "Health                        39\n",
      "Environment                   24\n",
      "Emergency Preparedness        11\n",
      "Name: category_desc, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "volunteer_X = volunteer.drop(\"category_desc\", axis=1)\n",
    "volunteer_y = volunteer[[\"category_desc\"]]\n",
    "\n",
    "# Use stratified sampling to split up the dataset according to the class distribution\n",
    "X_train, X_test, y_train, y_test = train_test_split(volunteer_X, volunteer_y, stratify=volunteer_y)\n",
    "print(y_train[\"category_desc\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical feature encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['population', 'fertility', 'HIV', 'CO2', 'BMI_male', 'GDP',\n",
      "       'BMI_female', 'life', 'child_mortality', 'Region_America',\n",
      "       'Region_East Asia & Pacific', 'Region_Europe & Central Asia',\n",
      "       'Region_Middle East & North Africa', 'Region_South Asia',\n",
      "       'Region_Sub-Saharan Africa'],\n",
      "      dtype='object')\n",
      "Index(['population', 'fertility', 'HIV', 'CO2', 'BMI_male', 'GDP',\n",
      "       'BMI_female', 'life', 'child_mortality', 'Region_East Asia & Pacific',\n",
      "       'Region_Europe & Central Asia', 'Region_Middle East & North Africa',\n",
      "       'Region_South Asia', 'Region_Sub-Saharan Africa'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('gm_2008_region.csv')\n",
    "# Create dummy variables: df_region\n",
    "df_region = pd.get_dummies(df)\n",
    "print(df_region.columns)\n",
    "\n",
    "# Create dummy variables without region_America\n",
    "df_region = pd.get_dummies(df, drop_first=True)\n",
    "print(df_region.columns)\n",
    "\n",
    "# Append design matrix with dummy variables for categorical variable\n",
    "X = df.drop(['life', 'Region'], axis=1)\n",
    "X = np.append(X, df_region, axis=1)\n",
    "y = df['life']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    }
   ],
   "source": [
    "# Use Pandas to encode us values as 1 and others as 0\n",
    "ufo[\"country_enc\"] = ufo[\"country\"].apply(lambda val: 1 if val == \"us\" else 0)\n",
    "\n",
    "# Print the number of unique type values\n",
    "print(len(ufo[\"type\"].unique()))\n",
    "\n",
    "# Create a one-hot encoded set of the type values\n",
    "type_set = pd.get_dummies(ufo[\"type\"])\n",
    "\n",
    "# Concatenate this set back to the ufo DataFrame\n",
    "ufo = pd.concat([ufo, type_set], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  MSZoning Neighborhood BldgType HouseStyle PavedDrive\n",
      "0       RL      CollgCr     1Fam     2Story          Y\n",
      "1       RL      Veenker     1Fam     1Story          Y\n",
      "2       RL      CollgCr     1Fam     2Story          Y\n",
      "3       RL      Crawfor     1Fam     2Story          Y\n",
      "4       RL      NoRidge     1Fam     2Story          Y\n",
      "   MSZoning  Neighborhood  BldgType  HouseStyle  PavedDrive\n",
      "0         3             5         0           5           2\n",
      "1         3            24         0           2           2\n",
      "2         3             5         0           5           2\n",
      "3         3             6         0           5           2\n",
      "4         3            15         0           5           2\n"
     ]
    }
   ],
   "source": [
    "# Ames housing data (unprocessed)\n",
    "df = pd.read_csv('ames_unprocessed_data.csv')\n",
    "\n",
    "# label encoding\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Fill missing values with 0\n",
    "df.LotFrontage = df.LotFrontage.fillna(0)\n",
    "\n",
    "# Create a boolean mask for categorical columns\n",
    "categorical_mask = (df.dtypes == object)\n",
    "\n",
    "# Get list of categorical column names\n",
    "categorical_columns = df.columns[categorical_mask].tolist()\n",
    "print(df[categorical_columns].head())\n",
    "\n",
    "le = LabelEncoder()\n",
    "df[categorical_columns] = df[categorical_columns].apply(lambda x: le.fit_transform(x))\n",
    "\n",
    "print(df[categorical_columns].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1460, 21)\n",
      "(1460, 62)\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# One hot encoding\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ohe = OneHotEncoder(categorical_features=categorical_mask,sparse=False)\n",
    "# ouptput = Numpy array!\n",
    "df_encoded = ohe.fit_transform(df)\n",
    "\n",
    "# Print the shape \n",
    "print(df.shape)\n",
    "print(df_encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age        9\n",
      "bp        12\n",
      "sg        47\n",
      "al        46\n",
      "su        49\n",
      "bgr      152\n",
      "bu        65\n",
      "sc         4\n",
      "sod        4\n",
      "pot       44\n",
      "hemo      19\n",
      "pcv       17\n",
      "wc        87\n",
      "rc        88\n",
      "rbc       52\n",
      "pc        71\n",
      "pcc      106\n",
      "ba       131\n",
      "htn        2\n",
      "dm         2\n",
      "cad        2\n",
      "appet      1\n",
      "pe         1\n",
      "ane        1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn_pandas import CategoricalImputer\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "df = pd.read_csv('chronic_kidney_disease.csv', header=None)\n",
    "df.columns = ['age', 'bp', 'sg', 'al', 'su', 'bgr',\n",
    "              'bu', 'sc', 'sod', 'pot', 'hemo', 'pcv',\n",
    "              'wc', 'rc', 'rbc', 'pc', 'pcc', 'ba',\n",
    "              'htn', 'dm', 'cad', 'appet', 'pe', 'ane','ckd']\n",
    "X, y = df.iloc[:,:-1], df.iloc[:,-1]\n",
    "df[df == \"?\"] = np.nan\n",
    "\n",
    "for i in range(0,5):\n",
    "    X.iloc[:,i]=X.iloc[:,i].astype(float)\n",
    "for i in range(9,18):\n",
    "    X.iloc[:,i]=X.iloc[:,i].astype(float)\n",
    "\n",
    "# Check number of nulls in each feature column\n",
    "nulls_per_column = X.isnull().sum()\n",
    "print(nulls_per_column)\n",
    "\n",
    "# Create a boolean mask for categorical columns\n",
    "categorical_feature_mask = X.dtypes == object\n",
    "categorical_columns = X.columns[categorical_feature_mask].tolist()\n",
    "non_categorical_columns = X.columns[~categorical_feature_mask].tolist()\n",
    "\n",
    "# Apply numeric imputer\n",
    "numeric_imputation_mapper = DataFrameMapper(\n",
    "                                            [([numeric_feature],SimpleImputer(strategy=\"median\")) \n",
    "                                             for numeric_feature in non_categorical_columns],\n",
    "                                            input_df=True,\n",
    "                                            df_out=True\n",
    "                                           )\n",
    "# Apply categorical imputer\n",
    "categorical_imputation_mapper = DataFrameMapper(\n",
    "                                                [(category_feature, CategoricalImputer()) \n",
    "                                                 for category_feature in categorical_columns],\n",
    "                                                input_df=True,\n",
    "                                                df_out=True\n",
    "                                               )\n",
    "# Combine the numeric and categorical transformations\n",
    "numeric_categorical_union = FeatureUnion([\n",
    "                                          (\"num_mapper\", numeric_imputation_mapper),\n",
    "                                          (\"cat_mapper\", categorical_imputation_mapper)\n",
    "                                         ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    democrat       0.97      0.97      0.97        36\n",
      "  republican       0.97      0.97      0.97        34\n",
      "\n",
      "   micro avg       0.97      0.97      0.97        70\n",
      "   macro avg       0.97      0.97      0.97        70\n",
      "weighted avg       0.97      0.97      0.97        70\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "y = votes.loc[:,'party'] \n",
    "X = votes.drop('party', axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Setup the pipeline steps: steps\n",
    "steps = [('imputation', SimpleImputer(missing_values='NaN', strategy='most_frequent')),\n",
    "        ('SVM', SVC())]\n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "# Fit the pipeline to the train set\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test set\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Compute metrics\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with Scaling: 0.9714285714285714\n",
      "Accuracy without Scaling: 0.9714285714285714\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary modules\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "steps = [('scaler', StandardScaler()),\n",
    "        ('knn', KNeighborsClassifier())]\n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Fit the pipeline to the training set\n",
    "knn_scaled = pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Instantiate and fit a k-NN classifier to the unscaled data\n",
    "knn_unscaled = KNeighborsClassifier().fit(X_train, y_train)\n",
    "\n",
    "# Compute and print metrics\n",
    "print('Accuracy with Scaling: {}'.format(knn_scaled.score(X_test, y_test)))\n",
    "print('Accuracy without Scaling: {}'.format(knn_unscaled.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
