{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Vincent/Desktop/Python/DataCamp/Data\n"
     ]
    }
   ],
   "source": [
    "%cd '/Users/Vincent/Desktop/Python/DataCamp/Data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss minimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "740.7025\n",
      "5.9827175\n",
      "8.582166\n",
      "2.826127\n",
      "2.6676488\n"
     ]
    }
   ],
   "source": [
    "# Linear Regression loss minimization\n",
    "housing = pd.read_csv(\"kc_house_data.csv\")\n",
    "size_log = np.log(housing['sqft_lot'])\n",
    "price_log = np.log(housing['price'])\n",
    "bedrooms = housing['bedrooms']\n",
    "\n",
    "# Define a linear regression model\n",
    "def linear_regression(intercept, slope, features):\n",
    "\treturn intercept + slope*features\n",
    "\n",
    "# Set loss_function() to take the variables as arguments\n",
    "def loss_function(intercept, slope, features, targets):\n",
    "\t# Set the predicted values\n",
    "\tpredictions = linear_regression(intercept, slope, features)\n",
    "    \n",
    "    # Return the mean squared error loss\n",
    "\treturn keras.losses.mse(targets,predictions)\n",
    "\n",
    "# Initialize an adam optimizer\n",
    "opt = keras.optimizers.Adam(0.5)\n",
    "\n",
    "slope, intercept = tf.Variable(5, dtype=float), tf.Variable(0.01, dtype=float)\n",
    "\n",
    "for j in range(100):\n",
    "\t# Apply minimize, pass the loss function, and supply the variables\n",
    "\topt.minimize(lambda: loss_function(intercept, slope, features=size_log, targets=price_log)\n",
    "                 , var_list=[intercept, slope])\n",
    "\n",
    "\t# Print every 20th value of the loss\n",
    "\tif j % 20 == 0:\n",
    "\t\tprint(loss_function(intercept, slope,features= size_log, targets=price_log).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss minimal at last iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 12.430, intercept: 0.101, slope_1: 0.050, slope_2: 0.020\n",
      "loss: 12.429, intercept: 0.102, slope_1: 0.050, slope_2: 0.020\n",
      "loss: 12.428, intercept: 0.103, slope_1: 0.050, slope_2: 0.020\n",
      "loss: 12.427, intercept: 0.104, slope_1: 0.050, slope_2: 0.020\n",
      "loss: 12.426, intercept: 0.105, slope_1: 0.050, slope_2: 0.020\n"
     ]
    }
   ],
   "source": [
    "# Multiple linear regression loss minimization\n",
    "def linear_regression(params, feature1 = size_log, feature2 = bedrooms):\n",
    "\treturn params[0] + feature1*params[1] + feature2*params[2]\n",
    "\n",
    "# Define the loss function\n",
    "def loss_function_multiple(params, targets = price_log, feature1 = size_log, feature2 = bedrooms):\n",
    "\t# Set the predicted values\n",
    "\tpredictions = linear_regression(params, feature1, feature2)\n",
    "  \n",
    "\t# Use the mean absolute error loss\n",
    "\treturn keras.losses.mae(targets, predictions)\n",
    "\n",
    "def print_results(params):\n",
    "    return print('loss: {:0.3f}'\\\n",
    "                 ', intercept: {:0.3f}'\\\n",
    "                 ', slope_1: {:0.3f}'\\\n",
    "                 ', slope_2: {:0.3f}'.format(loss_function(params).numpy()\n",
    "                                             , params[0].numpy(), params[1].numpy()\n",
    "                                             , params[2].numpy()))\n",
    "\n",
    "\n",
    "# Define the optimize operation\n",
    "opt = keras.optimizers.Adam()\n",
    "\n",
    "params = tf.Variable([0.1 , 0.05, 0.02], dtype=float)\n",
    "\n",
    "# Perform minimization and print trainable variables\n",
    "for j in range(5):\n",
    "\topt.minimize(lambda: loss_function_multiple(params), var_list=[params])\n",
    "\tprint_results(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.680509 1.5169338\n"
     ]
    }
   ],
   "source": [
    "# Initialize adam optimizer\n",
    "opt = keras.optimizers.Adam()\n",
    "\n",
    "# Load data in batches\n",
    "for batch in pd.read_csv('kc_house_data.csv', chunksize=100):\n",
    "\tsize_batch = np.array(batch['sqft_lot'], np.float32)\n",
    "\n",
    "\t# Extract the price values for the current batch\n",
    "\tprice_batch = np.array(batch['price'], np.float32)\n",
    "\n",
    "\t# Complete the loss, fill in the variable list, and minimize\n",
    "\topt.minimize(lambda: loss_function(intercept, slope, price_batch, size_batch)\n",
    "                 , var_list=[intercept, slope])\n",
    "\n",
    "# Print trained parameters\n",
    "print(intercept.numpy(), slope.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " prediction: 0.9525741338729858\n",
      "\n",
      " actual: 1\n",
      "\n",
      " shape of features:  (1, 3)\n",
      "\n",
      " shape of weights:  (3, 2)\n",
      "\n",
      " shape of bias:  ()\n",
      "\n",
      " shape of activation layer:  (1, 2)\n"
     ]
    }
   ],
   "source": [
    "# Construct neural net with one hidden layer from scratch\n",
    "# for a single observation for now\n",
    "bias = tf.Variable(1.0)\n",
    "weights = tf.Variable(tf.ones((3, 2)))\n",
    "\n",
    "# 1 single observation for now\n",
    "features = np.array([ 2.,  1., 24.],dtype='float32').reshape(1,-1)\n",
    "\n",
    "product = tf.matmul(features, weights)\n",
    "activation = keras.activations.sigmoid(product + bias)\n",
    "\n",
    "# Initialize bias2 and weights2\n",
    "bias_hidden = tf.Variable(1.0)\n",
    "weights_hidden = tf.Variable(tf.ones((2, 1)))\n",
    "\n",
    "# Perform matrix multiplication of dense1 and weights2\n",
    "product_hidden = tf.matmul(activation,weights_hidden)\n",
    "\n",
    "# Apply activation to product2 + bias2 and print the prediction\n",
    "prediction = keras.activations.sigmoid(product_hidden + bias_hidden)\n",
    "print('\\n prediction: {}'.format(prediction.numpy()[0,0]))\n",
    "print('\\n actual: 1')\n",
    "\n",
    "# Print the shapes of borrower_features, weights1, bias1, and dense1\n",
    "print('\\n shape of features: ', features.shape)\n",
    "print('\\n shape of weights: ', weights.shape)\n",
    "print('\\n shape of bias: ', bias.shape)\n",
    "print('\\n shape of activation layer: ', activation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Errors: \n",
      " [[ 0.5]\n",
      " [ 0.5]\n",
      " [-0.5]\n",
      " [-0.5]\n",
      " [-0.5]]\n"
     ]
    }
   ],
   "source": [
    "# Classification\n",
    "credit = pd.read_csv('uci_credit_card.csv')\n",
    "bill_amounts = np.array(credit.loc[:,['BILL_AMT1','BILL_AMT2','BILL_AMT3']])\n",
    "default = np.array(credit.iloc[:,-1]).reshape(-1,1)\n",
    "\n",
    "# Construct input layer from features\n",
    "inputs = tf.constant(bill_amounts, dtype='float32')\n",
    "\n",
    "# first dense layer\n",
    "dense1 = keras.layers.Dense(3, activation='relu')(inputs)\n",
    "# second dense layer\n",
    "dense2 = keras.layers.Dense(2, activation='relu')(dense1)\n",
    "# output layer\n",
    "outputs = keras.layers.Dense(1, activation='sigmoid')(dense2)\n",
    "\n",
    "# Print error for first five examples\n",
    "error = default[:5] - outputs.numpy()[:5]\n",
    "print(\"Errors: \\n\", error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.26324698 0.05705299 0.20492303 0.06891117 0.17117931 0.23468648]\n",
      " [0.35041356 0.08097418 0.12154561 0.06995234 0.23805432 0.13905996]\n",
      " [0.26676628 0.08445688 0.11434913 0.08605982 0.30729672 0.1410712 ]\n",
      " [0.2402384  0.06995337 0.12828635 0.07499182 0.27226552 0.21426457]\n",
      " [0.27779093 0.0625894  0.10677237 0.05452311 0.2997077  0.19861647]]\n"
     ]
    }
   ],
   "source": [
    "# Muticlass classification\n",
    "# Construct input layer from borrower features\n",
    "features = np.array(credit.iloc[:,12:-1])\n",
    "\n",
    "inputs = tf.constant(features, dtype='float32')\n",
    "\n",
    "#  first dense layer\n",
    "dense1 = keras.layers.Dense(10, activation='sigmoid')(inputs)\n",
    "#  second dense layer\n",
    "dense2 = keras.layers.Dense(8, activation='relu')(dense1)\n",
    "#  output layer\n",
    "outputs = keras.layers.Dense(6, activation='softmax')(dense2)\n",
    "\n",
    "# Print first five predictions\n",
    "print(outputs.numpy()[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that each row of outputs sums to one since a row contains the predicted class probabilities for one example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7445114 0.24999999\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "pi = 3.141592653589793\n",
    "\n",
    "def loss_function(x):\n",
    "    return 4.0*math.cos(x-1) + tf.divide(math.cos(2.0*pi*x),x)\n",
    "\n",
    "# Initialize x_1 and x_2\n",
    "x_1 = tf.Variable(0.05,dtype='float32')\n",
    "x_2 = tf.Variable(0.05,dtype='float32')\n",
    "\n",
    "# Define the optimization operation for opt_1 and opt_2\n",
    "opt_1 = keras.optimizers.RMSprop(learning_rate=0.01, momentum=0.99)\n",
    "opt_2 = keras.optimizers.RMSprop(learning_rate=0.01, momentum=0.00)\n",
    "\n",
    "for j in range(100):\n",
    "\topt_1.minimize(lambda: loss_function(x_1), var_list=[x_1])\n",
    "    # Define the minimization operation for opt_2\n",
    "\topt_2.minimize(lambda: loss_function(x_2), var_list=[x_2])\n",
    "\n",
    "# Print x_1 and x_2 as numpy arrays\n",
    "print(x_1.numpy(), x_2.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen below, the optimization without momentum gets stuck in the local minimum of the energy function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Minima](minima.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training & Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "default = np.array(credit.iloc[:,-1], dtype=\"float32\").reshape(-1,1)\n",
    "features = np.array(credit.iloc[:,1:-1], dtype='float32')\n",
    "\n",
    "x=np.c_[features,default]\n",
    "np.random.shuffle(x)\n",
    "\n",
    "ratio = 2/3\n",
    "train_features = x[0:int(ratio*len(x))][:,:-1]\n",
    "test_features = x[int(ratio*len(x)):len(x)][:,:-1]\n",
    "train_labels = x[0:int(ratio*len(x))][:,-1]\n",
    "test_labels = x[int(ratio*len(x)):len(x)][:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "def model(w1, b1, w2, b2, features = train_features):\n",
    "\t# Apply relu activation functions to layer 1\n",
    "\tlayer1 = keras.activations.relu(tf.matmul(features, w1) + b1)\n",
    "    # Apply dropout\n",
    "\tdropout = keras.layers.Dropout(0.25)(layer1)\n",
    "\treturn keras.activations.sigmoid(tf.matmul(dropout, w2) + b2)\n",
    "\n",
    "# Define the loss function\n",
    "def loss_function(w1, b1, w2, b2, features = train_features, targets = train_labels):\n",
    "\tpredictions = model(w1, b1, w2, b2)\n",
    "\t# Pass targets and predictions to the cross entropy loss\n",
    "\treturn keras.losses.binary_crossentropy(targets, predictions)\n",
    "\n",
    "# Define the layer 1 weights\n",
    "w1, b1 = tf.Variable(tf.random.normal([23, 7])), tf.Variable(tf.ones([7]))\n",
    "# Define the layer 2 weights\n",
    "w2, b2 = tf.Variable(tf.random.normal([7, 1])), tf.Variable(0.0)\n",
    "\n",
    "# Train the model\n",
    "for j in range(100):\n",
    "\topt.minimize(lambda: loss_function(w1, b1, w2, b2), \n",
    "                 var_list=[w1, b1, w2, b2])\n",
    "\n",
    "# Make predictions and evaluate\n",
    "model_predictions = model(w1, b1, w2, b2, test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78.03999999999999"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def accuracy(yp,y):\n",
    "    \n",
    "    y=y.reshape(-1,1)\n",
    "    e=yp-y\n",
    "    o=np.count_nonzero(e)\n",
    "    \n",
    "    return ((len(y)-o)/len(y))*100\n",
    "accuracy(model_predictions,test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model is about 78% accurate in its prediction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
